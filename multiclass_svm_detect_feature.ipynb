{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Multiclass SVM Classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7909fedaf0f8d4b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a093d515b7093878"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from skimage.feature import hog"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T12:44:47.321376100Z",
     "start_time": "2023-12-13T12:44:47.296958Z"
    }
   },
   "id": "404e8a20b7185ec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4934a5032c0d0a6"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "name_path = './archive/lfw_allnames.csv'\n",
    "images_path = './archive/lfw-deepfunneled/lfw-deepfunneled'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T12:44:47.323376400Z",
     "start_time": "2023-12-13T12:44:47.300356200Z"
    }
   },
   "id": "ab3aaaaa62e20034"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Only use classes with at least 100 images to simplify the problem."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22145e0a6d76dc39"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "df_names = pd.read_csv(name_path)\n",
    "min_faces_per_person = 100\n",
    "df_names = df_names.loc[df_names[\"images\"] >= min_faces_per_person, :]\n",
    "names = list(df_names[\"name\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T12:44:47.451905900Z",
     "start_time": "2023-12-13T12:44:47.301355400Z"
    }
   },
   "id": "4c9ed38c21cdaf13"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: Colin_Powell, number of samples: 236.\n",
      "Class: Donald_Rumsfeld, number of samples: 121.\n",
      "Class: George_W_Bush, number of samples: 530.\n",
      "Class: Gerhard_Schroeder, number of samples: 109.\n",
      "Class: Tony_Blair, number of samples: 144.\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "# load images\n",
    "for name in names:\n",
    "    dir_path = os.path.join(images_path, name)\n",
    "    list_images_name = os.listdir(dir_path)\n",
    "    for image_name in list_images_name:\n",
    "        image_path = os.path.join(dir_path, image_name)\n",
    "        img_rgb = plt.imread(image_path)\n",
    "        X.append(img_rgb)\n",
    "        Y.append(name)\n",
    "    print(f\"Class: {name}, number of samples: {len(list_images_name)}.\")\n",
    "X = np.asarray(X)\n",
    "Y = np.asarray(Y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T12:44:48.603999700Z",
     "start_time": "2023-12-13T12:44:47.455905400Z"
    }
   },
   "id": "d5d8bd505a2c2506"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "def create_features(img, show_hog=False):\n",
    "    # flatten image\n",
    "    color_features = img.flatten()\n",
    "    # convert image to greyscale\n",
    "    grey_image = np.array(0.299 * img[:, :, 0] + 0.587 * img[:, :, 1] + 0.114 * img[:, :, 2])\n",
    "    # get HOG features from greyscale image\n",
    "    hog_features, hog_image = hog(grey_image, visualize=True, block_norm='L2-Hys', pixels_per_cell=(16, 16))\n",
    "    if show_hog:\n",
    "        plt.imshow(hog_image, cmap=\"gray\")\n",
    "    # combine color and hog features into a single array\n",
    "    flat_features = np.hstack((color_features, hog_features))\n",
    "    return flat_features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T12:44:48.607604300Z",
     "start_time": "2023-12-13T12:44:48.605999900Z"
    }
   },
   "id": "b34685c27d5ba0b5"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "feature_matrix = []\n",
    "for x in X:\n",
    "    feature_matrix.append(create_features(x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T12:45:07.277076300Z",
     "start_time": "2023-12-13T12:44:48.608604600Z"
    }
   },
   "id": "17369b620cc0faa3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PCA\n",
    "Use PCA to reduce the data dimension."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6d0462efaa99ad"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape is:  (1140, 201189)\n",
      "PCA matrix shape is:  (1140, 500)\n"
     ]
    }
   ],
   "source": [
    "# get shape of feature matrix\n",
    "feature_matrix = np.asarray(feature_matrix)\n",
    "print('Feature matrix shape is: ', feature_matrix.shape)\n",
    "\n",
    "# define standard scaler\n",
    "ss = StandardScaler()\n",
    "# run this on our feature matrix\n",
    "bees_stand = ss.fit_transform(feature_matrix)\n",
    "\n",
    "pca = PCA(n_components=500)\n",
    "# use fit_transform to run PCA on our standardized matrix\n",
    "pca.fit(bees_stand)\n",
    "X_pca = pca.transform(bees_stand)\n",
    "print('PCA matrix shape is: ', X_pca.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T12:45:47.967794700Z",
     "start_time": "2023-12-13T12:45:07.278081400Z"
    }
   },
   "id": "7d153c2bdb794e43"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "# label_encoder object knows  \n",
    "# how to understand word labels. \n",
    "label_encoder = LabelEncoder() \n",
    "  \n",
    "# Encode labels in column 'species'. \n",
    "Y = label_encoder.fit_transform(Y) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T12:45:48.151938200Z",
     "start_time": "2023-12-13T12:45:48.115333200Z"
    }
   },
   "id": "665b523e21e243c"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "# split dataset\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_pca, Y, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T12:45:48.198226300Z",
     "start_time": "2023-12-13T12:45:48.136397500Z"
    }
   },
   "id": "7754662fcb3af05d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## One-Against-All Multiclass SVM Classifier\n",
    "One-against-all SVM is based on binary SVM [1]. The concept is that, for each class, a binary SVM is constructed by treating it as the positive class and all samples of the remaining N-1 classes as the negative class."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ece25d6b0b02e7fd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The mathematical formulations for the SVM hinge loss function and parameter updates are as follows.\n",
    "**Loss funtion:**\n",
    "$$L(\\mathbf{w},b)=\\frac1N\\sum_{i=1}^N\\max(0,1-y_i(\\mathbf{w}\\cdot\\mathbf{x}_i+b))$$\n",
    "In this loss function, if a sample point is correctly classified, the loss is 0, otherwise the loss is $1-y_i(\\mathbf{w}\\cdot\\mathbf{x}_i+b)$\n",
    "\n",
    "**Parameters update:**\n",
    "The mathematical formula for parameter updating, using gradient descent, is an update of the gradient of the loss function with respect to weight and bias.\n",
    "For weight $\\mathbf{w}$:\n",
    "$$\\mathbf{w}\\leftarrow\\mathbf{w}-\\alpha\\left(\\lambda\\mathbf{w}-\\frac1N\\sum_{i=1}^N1(1-y_i(\\mathbf{w}\\cdot\\mathbf{x}_i+b)>0)\\cdot y_i\\cdot\\mathbf{x}_i\\right)$$\n",
    "For bias $\\mathbf{b}$:\n",
    "$$b\\leftarrow b-\\alpha\\frac1N\\sum_{i=1}^N1(1-y_i(\\mathbf{w}\\cdot\\mathbf{x}_i+b)>0)\\cdot y_i$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b747fe0a7bac256"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9087719298245615\n"
     ]
    }
   ],
   "source": [
    "class SVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_=2, epochs=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.lambda_ = lambda_\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def train(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        unique_classes = np.unique(y)\n",
    "        num_classes = len(unique_classes)\n",
    "\n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.zeros((num_classes, num_features))\n",
    "        self.bias = np.zeros(num_classes)\n",
    "\n",
    "        # One-vs-All training\n",
    "        for c in range(num_classes):\n",
    "            binary_labels = np.where(y == unique_classes[c], 1, -1)\n",
    "\n",
    "            for epoch in range(self.epochs):\n",
    "                for i in range(num_samples):\n",
    "                    xi = X[i]\n",
    "                    yi = binary_labels[i]\n",
    "\n",
    "                    # Update weights and bias (simplified)\n",
    "                    if yi * (np.dot(self.weights[c], xi) + self.bias[c]) <= 1:\n",
    "                        self.weights[c] += self.learning_rate * (yi * xi - self.lambda_ * self.weights[c])\n",
    "                        self.bias[c] += self.learning_rate * yi\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions using the trained SVM\n",
    "        predictions = []\n",
    "        for xi in X:\n",
    "            scores = np.dot(self.weights, xi) + self.bias\n",
    "            predicted_class = np.argmax(scores)\n",
    "            predictions.append(predicted_class)\n",
    "        return np.array(predictions)\n",
    "\n",
    "\n",
    "# Create and train the SVM model\n",
    "svm_model = SVM()\n",
    "svm_model.train(X_train, Y_train)\n",
    "\n",
    "# Make predictions on test data\n",
    "test_predictions = svm_model.predict(X_test)\n",
    "accuracy = np.mean(test_predictions == Y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-13T12:45:54.701546800Z",
     "start_time": "2023-12-13T12:45:48.252414900Z"
    }
   },
   "id": "39ea6ef1b92fdc49"
  },
  {
   "cell_type": "markdown",
   "source": [
    "[1] C. W. Hsu and C. J. Lin. 2002. A comparison of methods for multi-class support vector machines. IEEE Trans. Neural Netw. 13, 2, 415-425."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1cd8b17d23f9367"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
